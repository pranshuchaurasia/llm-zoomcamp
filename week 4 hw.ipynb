{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e471cb-7aec-4c00-9eed-36e6f2447eb5",
   "metadata": {},
   "source": [
    "## Homework: Evaluation and Monitoring\n",
    "\n",
    "In this homework, we'll evaluate the quality of our RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd8e36-32c1-435a-be50-56618ee7a605",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll evaluate the quality of our RAG system with [gpt-4o-mini](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv) - only the first 300 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79da91e5-bb37-4a7a-860b-0d3909f5de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "                                          answer_llm  \\\n",
      "0  You can sign up for the course by visiting the...   \n",
      "1  You can sign up using the link provided in the...   \n",
      "2  Yes, there is an FAQ for the Machine Learning ...   \n",
      "\n",
      "                                         answer_orig  document  \\\n",
      "0  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
      "1  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
      "2  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
      "\n",
      "                                            question  \\\n",
      "0                Where can I sign up for the course?   \n",
      "1                 Can you provide a link to sign up?   \n",
      "2  Is there an FAQ for this Machine Learning course?   \n",
      "\n",
      "                      course  \n",
      "0  machine-learning-zoomcamp  \n",
      "1  machine-learning-zoomcamp  \n",
      "2  machine-learning-zoomcamp  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "df = df.iloc[:300]\n",
    "print(len(df))\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf3f79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).',\n",
       " 'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       " 'document': '0227b872',\n",
       " 'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = df.to_dict(orient='records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042aa10-abed-4009-b7af-769d8b477cd0",
   "metadata": {},
   "source": [
    "## Q1. Embeddings model\n",
    "\n",
    "Get the embeddings model `multi-qa-mpnet-base-dot-v1` from\n",
    "[the Sentence Transformer library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f61b1fe-f67a-4217-9fbe-5fd57eefd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc3023-1249-4436-b587-28543eec587e",
   "metadata": {},
   "source": [
    "Create the embeddings for the first LLM answer.\n",
    "\n",
    "What's the first value of the resulting vector?\n",
    "\n",
    "* -0.42 <--\n",
    "* -0.22\n",
    "* -0.02\n",
    "* 0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b36201-7fd6-4b3d-ab90-10d75e53e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_llm: You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_llm: -0.42244655\n"
     ]
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "print(\"answer_llm:\", answer_llm)\n",
    "embedding_llm = model.encode(answer_llm)\n",
    "print(\"embedding_llm:\", embedding_llm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d621f-f5e0-4c08-83a9-7468310f4e73",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the `evaluations` list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n",
    "* 21.67\n",
    "* 31.67 <--\n",
    "* 41.67\n",
    "* 51.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd33453-4885-4c59-a566-3f5f405e2622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610c945493f34b0e9fd64fecc5c6d0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.515987, 13.418402, 25.313255, 12.147415, 18.747736]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evaluations = []\n",
    "for doc in tqdm(documents):\n",
    "    answer_llm_embed = model.encode(doc['answer_llm'])\n",
    "    answer_orig_embed = model.encode(doc['answer_orig'])\n",
    "    dot_product = answer_llm_embed.dot(answer_orig_embed)\n",
    "    evaluations.append(dot_product)\n",
    "\n",
    "print(evaluations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa4418c-2034-4d87-a59d-7051518cdebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.67430877685547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.percentile(evaluations, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583dc56-81ee-44c4-8125-2522c6818198",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we \n",
    "\n",
    "* Compute the norm of a vector\n",
    "* Divide each element by this norm\n",
    "\n",
    "So, for vector `v`, it'll be `v / ||v||`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604ec74b-2b50-4ef7-8b3a-4d6b2d92658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    return 0 if norm==0 else (v / norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b1bf-b81c-4609-ab3b-57714d5a2cfa",
   "metadata": {},
   "source": [
    "Let's put it into a function and then compute dot product \n",
    "between normalized vectors. This will give us cosine similarity\n",
    "\n",
    "What's the 75% cosine in the scores?\n",
    "* 0.63\n",
    "* 0.73\n",
    "* 0.83 <--\n",
    "* 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8c0a8d-1be8-4959-9157-4ed505add535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8b33bc6e2643338cec5da311b5c5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5067539, 0.38854873, 0.7185989, 0.33726627, 0.5217923]\n"
     ]
    }
   ],
   "source": [
    "evaluations_norm = []\n",
    "for doc in tqdm(documents):\n",
    "    answer_llm_embed_norm = normalize_vector(model.encode(doc['answer_llm']))\n",
    "    answer_orig_embed_norm = normalize_vector(model.encode(doc['answer_orig']))\n",
    "    dot_product = answer_llm_embed_norm.dot(answer_orig_embed_norm)\n",
    "    evaluations_norm.append(dot_product)\n",
    "\n",
    "print(evaluations_norm[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e183950-ad63-4d05-aa54-6ce39c7cc202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8362348973751068\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(evaluations_norm, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b59fb-9f48-4213-a45d-38a8d68ddda7",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "\n",
    "Now we will explore an alternative metric - the ROUGE score.  \n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n",
    "```bash\n",
    "pip install rouge\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8ff8e-d0ea-40a6-b65a-2168fab5fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd7713-b0e7-48f5-aaaa-4767979cbbb2",
   "metadata": {},
   "source": [
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)\n",
    "\n",
    "```\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "```\n",
    "\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "* `rouge-1` - the overlap of unigrams,\n",
    "* `rouge-2` - bigrams,\n",
    "* `rouge-l` - the longest common subsequence\n",
    "\n",
    "What's the F score for `rouge-1`?\n",
    "- 0.35\n",
    "- 0.45 <--\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c38295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': \"Yes, all sessions are recorded, so if you miss one, you won't miss anything. You can catch up on the content later. Additionally, you can submit your questions in advance for office hours, and those sessions are also recorded.\",\n",
       " 'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       " 'document': '5170565b',\n",
       " 'question': 'Are sessions recorded if I miss one?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d6e5260-491b-4c0b-9ca1-917f6bfa1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.45454545454545453, 'p': 0.45454545454545453, 'f': 0.45454544954545456}, 'rouge-2': {'r': 0.21621621621621623, 'p': 0.21621621621621623, 'f': 0.21621621121621637}, 'rouge-l': {'r': 0.3939393939393939, 'p': 0.3939393939393939, 'f': 0.393939388939394}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(documents[10]['answer_llm'], documents[10]['answer_orig'])[0]\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a82e0-64a1-4332-be29-dc21faabb760",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "\n",
    "- 0.35 <--\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23e2f21-0b4b-45e9-9a5f-b7dd5a0e6230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35490034990035496\n"
     ]
    }
   ],
   "source": [
    "avg = np.mean([scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']])\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2936b97a-e412-4dee-b915-a36e8683745b",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "Now let's compute the score for all the records\n",
    "\n",
    "```\n",
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770eae84-eda4-4f2b-a244-35e68a14749f",
   "metadata": {},
   "source": [
    "Create a dataframe from them\n",
    "\n",
    "What's the agerage `rouge_2` across all the records?\n",
    "- 0.10\n",
    "- 0.20 <--\n",
    "- 0.30\n",
    "- 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256607f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_avg_score(scores):\n",
    "    rouge_1 = scores['rouge-1']['f']\n",
    "    rouge_2 = scores['rouge-2']['f']\n",
    "    rouge_l = scores['rouge-l']['f']   \n",
    "\n",
    "    return {'rouge_1': rouge_1, 'rouge_2': rouge_2, 'rouge_l': rouge_l} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10c067a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edce245075d47368279ac13fcca0ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge_1': 0.09523809178130524, 'rouge_2': 0.028169010918468917, 'rouge_l': 0.09523809178130524}, {'rouge_1': 0.12499999641113292, 'rouge_2': 0.05555555225694465, 'rouge_l': 0.09374999641113295}, {'rouge_1': 0.41558441095631643, 'rouge_2': 0.17777777313333343, 'rouge_l': 0.3896103849822905}, {'rouge_1': 0.2162162117421476, 'rouge_2': 0.047058819111419105, 'rouge_l': 0.18918918471512064}, {'rouge_1': 0.14207649881095297, 'rouge_2': 0.03389830142092829, 'rouge_l': 0.12021857531368524}]\n"
     ]
    }
   ],
   "source": [
    "evaluations_rouge = []\n",
    "for doc in tqdm(documents):\n",
    "    scores = rouge_scorer.get_scores(doc['answer_llm'], doc['answer_orig'])[0]\n",
    "    avg_score = rouge_avg_score(scores)\n",
    "    evaluations_rouge.append(avg_score)\n",
    "\n",
    "print(evaluations_rouge[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a03005-df48-463b-9240-68c2e64bbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>rouge_2</th>\n",
       "      <th>rouge_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.389610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.189189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142076</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.120219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rouge_1   rouge_2   rouge_l\n",
       "0  0.095238  0.028169  0.095238\n",
       "1  0.125000  0.055556  0.093750\n",
       "2  0.415584  0.177778  0.389610\n",
       "3  0.216216  0.047059  0.189189\n",
       "4  0.142076  0.033898  0.120219"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rouge_score = pd.DataFrame(evaluations_rouge)\n",
    "df_rouge_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d4ef4e-c759-42b5-a4c5-761581beca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20696501983423318"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rouge_score['rouge_2'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
